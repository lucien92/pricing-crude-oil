{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, RepeatVector, Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16498/545267995.py:12: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_week['Price'].fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "week = \"/home/lucien/Bureau/projet_ml_finance/datahub/wti-week_csv.csv\"\n",
    "daily = \"/home/lucien/Bureau/projet_ml_finance/datahub/wti-daily_csv.csv\"\n",
    "\n",
    "data_week = pd.read_csv(week, delimiter = ',')\n",
    "data_daily = pd.read_csv(daily, delimiter = ',')\n",
    "\n",
    "#format datime\n",
    "data_week['Date'] = pd.to_datetime(data_week['Date'])\n",
    "\n",
    "#on complète les valeurs de Price null avec la méthode forward fill\n",
    "\n",
    "data_week['Price'].fillna(method='ffill', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's split data\n",
    "data = data_daily['Price']\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Diviser les données en train et test\n",
    "X_train = data[:split_index]\n",
    "X_test = data[split_index:]\n",
    "\n",
    "split_index = int(len(X_test) * 0.8)\n",
    "X_val= X_test[:split_index]\n",
    "X_test = X_test[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (6596, 200)\n",
      "val shape (1156, 200)\n",
      "test shape (137, 200)\n"
     ]
    }
   ],
   "source": [
    "# Supposons que X_train et X_test sont vos séries temporelles\n",
    "# Création des échantillons et des étiquettes pour l'entraînement\n",
    "def create_samples(series, lookback=30, horizon=5):\n",
    "    samples, labels = [], []\n",
    "    for i in range(len(series) - lookback - horizon + 1):\n",
    "        samples.append(series[i : i + lookback])  # Séquences temporelles précédentes (caractéristiques)\n",
    "        labels.append(series[i + lookback : i + lookback + horizon])  # Valeurs futures à prédire (étiquettes)\n",
    "    return np.array(samples), np.array(labels)\n",
    "\n",
    "lookback = 200  # Nombre de pas de temps précédents à utiliser comme caractéristiques\n",
    "horizon = 5   # Nombre de pas de temps à prédire dans le futur\n",
    "\n",
    "X_train_samples, y_train_labels = create_samples(X_train, lookback, horizon)\n",
    "X_val_samples, y_val_labels = create_samples(X_val, lookback, horizon)\n",
    "X_test_samples, y_test_labels = create_samples(X_test, lookback, horizon)\n",
    "\n",
    "print(\"train shape\", X_train_samples.shape)\n",
    "print(\"val shape\", X_val_samples.shape)\n",
    "print(\"test shape\", X_test_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape (6596, 200, 1)\n",
      "Validation set shape (1156, 200, 1)\n",
      "test set shape (137, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# #on reshape en ajoutant le nombre de feautures: Reshape from [samples, timesteps] into [samples, timesteps, features].\n",
    "\n",
    "X_train_series = X_train_samples.reshape((X_train_samples.shape[0], X_train_samples.shape[1], 1))\n",
    "X_val_series = X_val_samples.reshape((X_val_samples.shape[0], X_val_samples.shape[1], 1))\n",
    "X_test_series = X_test_samples.reshape((X_test_samples.shape[0], X_test_samples.shape[1], 1))\n",
    "print('Train set shape', X_train_series.shape)\n",
    "print('Validation set shape', X_val_series.shape)\n",
    "print('test set shape', X_test_series.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape (6596, 5, 40, 1)\n",
      "Validation set shape (1156, 5, 40, 1)\n",
      "test set shape (137, 5, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "subsequences = 5 #chaque séquence est divisée en plusieurs sous séquences de 2 étapes temporelles chacune, le CNN va d'abord extraire les features de chacune des sous séries\n",
    "\n",
    "timesteps = X_train_series.shape[1]//subsequences\n",
    "X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))\n",
    "X_val_series_sub = X_val_series.reshape((X_val_series.shape[0], subsequences, timesteps, 1))\n",
    "X_test_series_sub = X_test_series.reshape((X_test_series.shape[0], subsequences, timesteps, 1))\n",
    "print('Train set shape', X_train_series_sub.shape)\n",
    "print('Validation set shape', X_val_series_sub.shape)\n",
    "print('test set shape', X_test_series_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "epochs = 40\n",
    "batch = 25\n",
    "lr = 0.003\n",
    "adam = optimizers.Adam(lr)\n",
    "\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm.add(LSTM(50, activation='relu'))\n",
    "model_cnn_lstm.add(Dense(1))\n",
    "model_cnn_lstm.compile(loss='mse', optimizer=adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "207/207 - 3s - loss: 133.6597 - val_loss: 74.2777 - 3s/epoch - 14ms/step\n",
      "Epoch 2/40\n",
      "207/207 - 1s - loss: 40.3066 - val_loss: 40.7112 - 1s/epoch - 5ms/step\n",
      "Epoch 3/40\n",
      "207/207 - 1s - loss: 17.5141 - val_loss: 18.4274 - 1s/epoch - 5ms/step\n",
      "Epoch 4/40\n",
      "207/207 - 2s - loss: 12.0816 - val_loss: 49.6258 - 2s/epoch - 7ms/step\n",
      "Epoch 5/40\n",
      "207/207 - 1s - loss: 9.5100 - val_loss: 7.7236 - 1s/epoch - 7ms/step\n",
      "Epoch 6/40\n",
      "207/207 - 1s - loss: 8.0492 - val_loss: 30.8115 - 1s/epoch - 5ms/step\n",
      "Epoch 7/40\n",
      "207/207 - 1s - loss: 7.8938 - val_loss: 7.4433 - 1s/epoch - 5ms/step\n",
      "Epoch 8/40\n",
      "207/207 - 1s - loss: 8.7567 - val_loss: 7.9872 - 1s/epoch - 5ms/step\n",
      "Epoch 9/40\n",
      "207/207 - 1s - loss: 6.8397 - val_loss: 8.8842 - 1s/epoch - 5ms/step\n",
      "Epoch 10/40\n",
      "207/207 - 1s - loss: 6.2868 - val_loss: 18.2019 - 1s/epoch - 5ms/step\n",
      "Epoch 11/40\n",
      "207/207 - 1s - loss: 7.2155 - val_loss: 16.8537 - 1s/epoch - 5ms/step\n",
      "Epoch 12/40\n",
      "207/207 - 1s - loss: 6.4716 - val_loss: 9.5169 - 1s/epoch - 5ms/step\n",
      "Epoch 13/40\n",
      "207/207 - 1s - loss: 6.0858 - val_loss: 9.5558 - 1s/epoch - 5ms/step\n",
      "Epoch 14/40\n",
      "207/207 - 1s - loss: 6.1667 - val_loss: 5.5937 - 1s/epoch - 5ms/step\n",
      "Epoch 15/40\n",
      "207/207 - 1s - loss: 6.0338 - val_loss: 5.5853 - 1s/epoch - 5ms/step\n",
      "Epoch 16/40\n",
      "207/207 - 1s - loss: 10.5397 - val_loss: 21.8132 - 1s/epoch - 5ms/step\n",
      "Epoch 17/40\n",
      "207/207 - 1s - loss: 7.2034 - val_loss: 5.2214 - 1s/epoch - 5ms/step\n",
      "Epoch 18/40\n",
      "207/207 - 1s - loss: 6.4692 - val_loss: 5.0896 - 1s/epoch - 5ms/step\n",
      "Epoch 19/40\n",
      "207/207 - 1s - loss: 6.2728 - val_loss: 5.7427 - 1s/epoch - 5ms/step\n",
      "Epoch 20/40\n",
      "207/207 - 1s - loss: 6.6458 - val_loss: 5.8322 - 1s/epoch - 5ms/step\n",
      "Epoch 21/40\n",
      "207/207 - 1s - loss: 5.6094 - val_loss: 10.4115 - 1s/epoch - 5ms/step\n",
      "Epoch 22/40\n",
      "207/207 - 1s - loss: 5.6402 - val_loss: 5.8923 - 1s/epoch - 5ms/step\n",
      "Epoch 23/40\n",
      "207/207 - 1s - loss: 5.9252 - val_loss: 4.9661 - 1s/epoch - 5ms/step\n",
      "Epoch 24/40\n",
      "207/207 - 1s - loss: 5.2041 - val_loss: 18.9200 - 1s/epoch - 5ms/step\n",
      "Epoch 25/40\n",
      "207/207 - 1s - loss: 5.2556 - val_loss: 4.7307 - 1s/epoch - 5ms/step\n",
      "Epoch 26/40\n",
      "207/207 - 1s - loss: 6.6212 - val_loss: 9.2628 - 1s/epoch - 5ms/step\n",
      "Epoch 27/40\n",
      "207/207 - 1s - loss: 5.1635 - val_loss: 4.7486 - 1s/epoch - 5ms/step\n",
      "Epoch 28/40\n",
      "207/207 - 1s - loss: 5.3016 - val_loss: 5.4713 - 1s/epoch - 5ms/step\n",
      "Epoch 29/40\n",
      "207/207 - 1s - loss: 6.9468 - val_loss: 4.9326 - 1s/epoch - 5ms/step\n",
      "Epoch 30/40\n",
      "207/207 - 1s - loss: 5.5637 - val_loss: 7.5860 - 1s/epoch - 5ms/step\n",
      "Epoch 31/40\n",
      "207/207 - 1s - loss: 5.7293 - val_loss: 4.6568 - 1s/epoch - 5ms/step\n",
      "Epoch 32/40\n",
      "207/207 - 1s - loss: 4.9017 - val_loss: 5.3486 - 1s/epoch - 5ms/step\n",
      "Epoch 33/40\n",
      "207/207 - 1s - loss: 5.6400 - val_loss: 4.7829 - 1s/epoch - 5ms/step\n",
      "Epoch 34/40\n",
      "207/207 - 1s - loss: 4.9193 - val_loss: 4.8201 - 1s/epoch - 5ms/step\n",
      "Epoch 35/40\n",
      "207/207 - 1s - loss: 5.6403 - val_loss: 5.0028 - 1s/epoch - 5ms/step\n",
      "Epoch 36/40\n",
      "207/207 - 1s - loss: 5.6627 - val_loss: 7.3911 - 1s/epoch - 5ms/step\n",
      "Epoch 37/40\n",
      "207/207 - 1s - loss: 5.0960 - val_loss: 5.6424 - 1s/epoch - 5ms/step\n",
      "Epoch 38/40\n",
      "207/207 - 1s - loss: 5.0790 - val_loss: 24.3091 - 1s/epoch - 5ms/step\n",
      "Epoch 39/40\n",
      "207/207 - 1s - loss: 5.3876 - val_loss: 8.5986 - 1s/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "207/207 - 1s - loss: 5.0700 - val_loss: 5.9305 - 1s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_lstm_history = model_cnn_lstm.fit(X_train_series_sub, y_train_labels, validation_data=(X_val_series_sub, y_val_labels), epochs=40, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "Mean Percentage Error on Test Set: 3.42%\n"
     ]
    }
   ],
   "source": [
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = model_cnn_lstm.predict(X_test_series_sub)\n",
    "\n",
    "# Fonction pour calculer la Mean Percentage Error (MPE)\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Calcul de la MPE sur l'ensemble de test\n",
    "mpe = mean_percentage_error(y_test_labels, y_pred)\n",
    "print(f\"Mean Percentage Error on Test Set: {mpe:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultats:\n",
    "\n",
    "-3,55% pour un CNN-LSTM avec 2 pour hyperparamètres des subsequence\n",
    "-3% pour un CNN-LSTM avec 5 pour hyperparamètres des subsequence (mais temps d'entrainement plus long)\n",
    "-3,46% pour 10\n",
    "\n",
    "Remarque: meilleur performance car en réduisant la taille de la sous séquence on capture des motifs temporels plus fins et plus complexes. Cependant si on augmente trop on risque de louper des motifs important pour la prédiciton donc attention à ne pas trop segmenter en petites sous séquences. Par exemple quand on choisit 10 cela signifie qu'on étudie que les motifs de taille 3 ce qui peut être trop limité dans le cadre de la prédiction de l'évolution du prix de crude oil."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
